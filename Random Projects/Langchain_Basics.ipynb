{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euvjhVqlBA4g",
        "outputId": "cc23751b-0fc0-4a5a-98f4-f281b17dc8f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv,find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "openai.api_key = os.getenv(\"OPEN_API_KEY\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M3NPhI6MBSm6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -qU langchain langchain-openai\n"
      ],
      "metadata": {
        "id": "HVFYQuexCFjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b27d84-9981-4fbb-ff55-fdab80d4b3c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "rq3Qhq0sCfJa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(api_key=os.getenv(\"OPEN_API_KEY\"))\n",
        "chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GpWAQYICk48",
        "outputId": "534fd9b4-06c7-46dc-87c1-bd63b3667a99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x797bcdea9840>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x797bcdeab940>, root_client=<openai.OpenAI object at 0x797be7e0b250>, root_async_client=<openai.AsyncOpenAI object at 0x797bcdea98a0>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(api_key=os.getenv(\"OPEN_API_KEY\"),model=\"gpt-4o-mini\",temperature=0.0,)\n",
        "chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xg3bD1VDgKq",
        "outputId": "1cce8a01-5d12-4364-8328-0884bd12cbc5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x797bcdec1db0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x797bcdec3eb0>, root_client=<openai.OpenAI object at 0x797bcdeaba00>, root_async_client=<openai.AsyncOpenAI object at 0x797bcdec1e10>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template_string = \"\"\"Translate the text which is demlimited by triple backtacks into the style that is {style} \\\n",
        "text: ```{text}```  \"\"\""
      ],
      "metadata": {
        "id": "CddDM6p0D2V6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "prompt_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hNg9symETnY",
        "outputId": "51c285f0-85bd-401e-8bed-69644dc04fa9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text which is demlimited by triple backtacks into the style that is {style} text: ```{text}```  '), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTec3SAiEbJa",
        "outputId": "aaceb55c-3521-4e9a-9bea-9677f0aa7861"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['style', 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.messages[0].prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDJmW9lPE0hU",
        "outputId": "4dbe6bcd-d397-4ce3-bbc8-a8235cb51679"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text which is demlimited by triple backtacks into the style that is {style} text: ```{text}```  ')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.messages[0].prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CbxCdE0E95Q",
        "outputId": "563eabe7-e892-472b-ca0d-98d23a41746f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['style', 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_style = \"\"\" American English \\\n",
        "in a calm, professional and respectfull tone\"\"\"\n",
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse, \\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qVXvgg5zFJb6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_message = prompt_template.format_messages(style=customer_style,text=customer_email)\n",
        "customer_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44GQe5VDFWej",
        "outputId": "e2f8b3e0-e91a-4927-a500-b86e8c1b327a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"Translate the text which is demlimited by triple backtacks into the style that is  American English in a calm, professional and respectfull tone text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```  \", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(type(customer_message[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHzVXAz6FuGp",
        "outputId": "b27d1030-2c2c-4122-b1a4-3098b4c1958b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.messages.human.HumanMessage'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_response = chat(customer_message)\n",
        "customer_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeJ4NOaAID0h",
        "outputId": "a6284415-4103-48ae-a490-bdb6d5e03ff7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-2a05219021cc>:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  customer_response = chat(customer_message)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I am quite frustrated that the lid of my blender came off and splattered smoothie all over my kitchen walls. To make matters worse, the warranty does not cover the cost of cleaning up my kitchen. I would appreciate your assistance with this issue. Thank you.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 92, 'total_tokens': 144, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-aa0060ba-b4ce-4e18-bce5-fe13b2ea9355-0', usage_metadata={'input_tokens': 92, 'output_tokens': 52, 'total_tokens': 144, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "DKioHX7kIUWQ",
        "outputId": "fe6cd420-2cde-40dd-df95-f960804e7478"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am quite frustrated that the lid of my blender came off and splattered smoothie all over my kitchen walls. To make matters worse, the warranty does not cover the cost of cleaning up my kitchen. I would appreciate your assistance with this issue. Thank you.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_reply = \"\"\"Hey there customer, \\\n",
        "the warranty does not cover \\\n",
        "cleaning expenses for your kitchen \\\n",
        "because it's your fault that \\\n",
        "you misused your blender \\\n",
        "by forgetting to put the lid on before \\\n",
        "starting the blender. \\\n",
        "Tough luck! See ya!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WYtjFsn4IZGg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_style_pirate = \"\"\"a Polite tone that speaks in English Pirate \"\"\""
      ],
      "metadata": {
        "id": "BK6DGe9oJWCX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_message = prompt_template.format_messages(style=service_style_pirate,text=service_reply)"
      ],
      "metadata": {
        "id": "QgD3n-30JhVq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_response = chat(service_message)\n",
        "service_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwfiNXzXJtz0",
        "outputId": "a93a53ad-28cd-4e41-c9ce-7ef4a4027f72"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Ahoy there, esteemed customer! I must inform ye that the warranty be not coverin' the expenses for cleanin' yer kitchen, as it appears ye may have misused yer blender by forgettin' to secure the lid afore settin' it to whirl. Aye, 'tis a bit of tough luck, indeed! Wishing ye fair winds and following seas! Until we meet again!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 82, 'total_tokens': 163, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-423b96a0-14aa-4f72-bac8-cda04facf173-0', usage_metadata={'input_tokens': 82, 'output_tokens': 81, 'total_tokens': 163, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "SR_DzH67J2nd",
        "outputId": "687194f0-ebf3-4b9d-e054-ac1ee3a7d5c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Ahoy there, esteemed customer! I must inform ye that the warranty be not coverin' the expenses for cleanin' yer kitchen, as it appears ye may have misused yer blender by forgettin' to secure the lid afore settin' it to whirl. Aye, 'tis a bit of tough luck, indeed! Wishing ye fair winds and following seas! Until we meet again!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Parsers"
      ],
      "metadata": {
        "id": "PFVfCqxgVxpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want Output to be like this\n",
        "{\n",
        "  \"gift\": False,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHIZJb7bJ5D4",
        "outputId": "0c2e605c-b8ee-473e-8d35-50f90fb8727e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\"\n",
        "\n",
        "review_template = \"\"\"\"\n",
        "For the following text extract the following information.\n",
        "\n",
        "gift: Is the product is gift to someoneelse.\n",
        "Answer True if yes else False if no\n",
        "\n",
        "delivery_days: How many days it took the order to reach the customer ?\n",
        "If information is not found output -1\n",
        "\n",
        "price_value: Extract any information about the price or cost and \\\n",
        "output them as comma seperated python list\n",
        "\n",
        "Format the output in JSON format with the keys\n",
        "\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "1j8IP7jMNiX7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "review_message = prompt_template.format_messages(text=customer_review)\n",
        "review_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPIcbg7rOvW7",
        "outputId": "aac103b5-2d67-4653-9b58-8fc7ec5014ed"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='\"\\nFor the following text extract the following information.\\n\\ngift: Is the product is gift to someoneelse.\\nAnswer True if yes else False if no\\n\\ndelivery_days: How many days it took the order to reach the customer ?\\nIf information is not found output -1\\n\\nprice_value: Extract any information about the price or cost and output them as comma seperated python list\\n\\nFormat the output in JSON format with the keys\\n\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife\\'s anniversary present. I think my wife liked it so much she was speechless. So far I\\'ve been the only one using it, and I\\'ve been using it every other morning to clear the leaves on our lawn. It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\\n\\n ', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_response_1 = chat(review_message)\n",
        "review_response_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w69woj-7O6jJ",
        "outputId": "ac8efe2c-9bb8-4478-9aa5-221415d8a163"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='```json\\n{\\n  \"gift\": true,\\n  \"delivery_days\": 2,\\n  \"price_value\": []\\n}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 206, 'total_tokens': 232, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-4a41c5cf-268b-4a14-8e9b-92508d3455c9-0', usage_metadata={'input_tokens': 206, 'output_tokens': 26, 'total_tokens': 232, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(review_response_1.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaDmIKZlPK9x",
        "outputId": "23cdf5fe-a495-4cd1-932b-59c568ca6094"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"gift\": true,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": []\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(review_response_1.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzRMs0rUPQAo",
        "outputId": "6ed5505e-c615-439b-aa3b-7bbf83c613d0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "gift_schema = ResponseSchema(name=\"gift\",description=\"Was the item purchased\\\n",
        "                             as a gift for someone else? \\\n",
        "                             Answer True if yes,\\\n",
        "                             False if not or unknown.\")\n",
        "delivery_days_schema = ResponseSchema(name=\"delivery_days\",description=\"How many days\\\n",
        "                                      did it take for the product\\\n",
        "                                      to arrive? If this \\\n",
        "                                      information is not found,\\\n",
        "                                      output -1.\")\n",
        "price_value_schema = ResponseSchema(name=\"price_value\", description=\"Extract any\\\n",
        "                                    sentences about the value or \\\n",
        "                                    price, and output them as a \\\n",
        "                                    comma separated Python list.\")\n",
        "\n",
        "response_schema = [\n",
        "    gift_schema,\n",
        "    delivery_days_schema,\n",
        "    price_value_schema\n",
        "]\n"
      ],
      "metadata": {
        "id": "y0Pky_zARMKz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
        "output_parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MscLD8E8RX7V",
        "outputId": "5bde7cd4-0c8c-4d46-f502-dc66ba03d156"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructuredOutputParser(response_schemas=[ResponseSchema(name='gift', description='Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.', type='string'), ResponseSchema(name='delivery_days', description='How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.', type='string'), ResponseSchema(name='price_value', description='Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.', type='string')])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "pSCFPKdvUOJd",
        "outputId": "a3cb02aa-2566-4d87-bcf8-a8394854c5a8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\\n\\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\\n\\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\\n}\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_template_2 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product\\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "text: {text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template_2)\n",
        "review_message = prompt_template.format_messages(text=customer_review,format_instructions=format_instructions)"
      ],
      "metadata": {
        "id": "ePpYbQoWT0n_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_response = chat(review_message)\n",
        "review_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnDcBND_T_iK",
        "outputId": "600b3e37-718a-4cc3-eb86-b978100b0e89"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='```json\\n{\\n\\t\"gift\": \"True\",\\n\\t\"delivery_days\": \"2\",\\n\\t\"price_value\": \"It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\"\\n}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 324, 'total_tokens': 375, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-c62f07ea-42f4-434a-9b0a-df2ef9ad7442-0', usage_metadata={'input_tokens': 324, 'output_tokens': 51, 'total_tokens': 375, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(review_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7Ly4O2qUtE-",
        "outputId": "ef4413a0-39e1-4344-cf81-4b6df164262a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\t\"gift\": \"True\",\n",
            "\t\"delivery_days\": \"2\",\n",
            "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict = output_parser.parse(review_response.content)\n",
        "type(output_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cFcbQvFUxvf",
        "outputId": "9aa7632e-6662-4ece-a206-fd4116abe9b2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyUInyW3VAKK",
        "outputId": "1f49b7e3-fb9d-4b3f-8323-b6301cc8f3b5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': 'True',\n",
              " 'delivery_days': '2',\n",
              " 'price_value': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(review_response_1.content) # This is giving wrong answer to begin with"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U41oKXCVBij",
        "outputId": "5120ac94-67c4-470c-9dad-54c4b64b19ab"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"gift\": true,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": []\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "2De8Rhdt4ivX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "0MAMlMru4kTt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model='gpt-4o-mini',temperature=0.0, api_key=os.getenv(\"OPEN_API_KEY\"))\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFY8iz3h43xp",
        "outputId": "94314c51-a52d-487e-b688-c6eb2f8b91f9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-67461326b63b>:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "<ipython-input-41-67461326b63b>:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi My name is Manideep\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "z1RCeRqV5bJe",
        "outputId": "ee06e822-344d-4edd-ea5b-10c80a652ef8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi My name is Manideep\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello, Manideep! It's great to meet you! How's your day going so far?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is 1+1 \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "2QZS_T2j5svX",
        "outputId": "aba00abb-d377-4ea3-be52-ebf9ed8b3794"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi My name is Manideep\n",
            "AI: Hello, Manideep! It's great to meet you! How's your day going so far?\n",
            "Human: What is 1+1 \n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"1 + 1 equals 2! It's a simple yet fundamental math problem. If you have any more questions, whether they're about math or something else, feel free to ask!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "pUCXVK3h52bm",
        "outputId": "df10974e-a037-418c-a737-8d9d736f6c25"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi My name is Manideep\n",
            "AI: Hello, Manideep! It's great to meet you! How's your day going so far?\n",
            "Human: What is 1+1 \n",
            "AI: 1 + 1 equals 2! It's a simple yet fundamental math problem. If you have any more questions, whether they're about math or something else, feel free to ask!\n",
            "Human: What is my name\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Your name is Manideep! It's nice to know your name. If there's anything else you'd like to share or ask, I'm here to chat!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN_q4bb057K8",
        "outputId": "be98dbe1-26ba-4c2d-ca3f-38b4ede97bcb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi My name is Manideep', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, Manideep! It's great to meet you! How's your day going so far?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is 1+1 ', additional_kwargs={}, response_metadata={}), AIMessage(content=\"1 + 1 equals 2! It's a simple yet fundamental math problem. If you have any more questions, whether they're about math or something else, feel free to ask!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Your name is Manideep! It's nice to know your name. If there's anything else you'd like to share or ask, I'm here to chat!\", additional_kwargs={}, response_metadata={})])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-EMEFbEI6AFh",
        "outputId": "8862b2bf-17b1-4765-bec9-3a5b62004494"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Human: Hi My name is Manideep\\nAI: Hello, Manideep! It's great to meet you! How's your day going so far?\\nHuman: What is 1+1 \\nAI: 1 + 1 equals 2! It's a simple yet fundamental math problem. If you have any more questions, whether they're about math or something else, feel free to ask!\\nHuman: What is my name\\nAI: Your name is Manideep! It's nice to know your name. If there's anything else you'd like to share or ask, I'm here to chat!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2CV_6Nr6Dy0",
        "outputId": "77b77e9d-c1eb-41f4-ed54-565dc0c1c311"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi My name is Manideep\\nAI: Hello, Manideep! It's great to meet you! How's your day going so far?\\nHuman: What is 1+1 \\nAI: 1 + 1 equals 2! It's a simple yet fundamental math problem. If you have any more questions, whether they're about math or something else, feel free to ask!\\nHuman: What is my name\\nAI: Your name is Manideep! It's nice to know your name. If there's anything else you'd like to share or ask, I'm here to chat!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory1= ConversationBufferMemory()"
      ],
      "metadata": {
        "id": "pJYrbuVU6Nb1"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory1.save_context({\"input\": \"hi\"},{\"output\": \"Hi hello\"})"
      ],
      "metadata": {
        "id": "sODmEuql6Ymy"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory1.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "j2bJ5dXr6hHP",
        "outputId": "5f89463f-9ebd-4d27-ebac-74de85b47296"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: hi\\nAI: Hi hello'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory1.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFtDR1kH6iyh",
        "outputId": "cef8ae8f-3d7a-4f9d-9e84-55d7816c035f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: hi\\nAI: Hi hello'}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory1.save_context({\"input\": \"How are you\"},{\"output\": \"hot\"})"
      ],
      "metadata": {
        "id": "LPSSbnRM6mqg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory1.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "H48eKWs-6tcn",
        "outputId": "c5c64bc5-f446-47e9-a355-9c35562cc97a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: hi\\nAI: Hi hello\\nHuman: How are you\\nAI: hot'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory1.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JagS8-N6vEX",
        "outputId": "2b4b2a9c-fe11-41ce-ac24-4ca800efef92"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: hi\\nAI: Hi hello\\nHuman: How are you\\nAI: hot'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "# create a long string\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory2 = ConversationSummaryBufferMemory(llm=llm, max_token_limit=200)\n",
        "memory2.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory2.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory2.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D85lv6WF63yv",
        "outputId": "4f770a88-96b3-4001-d917-b1b214bdeb56"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-b08f3def659a>:12: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory2 = ConversationSummaryBufferMemory(llm=llm, max_token_limit=200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory2.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "5FRq8or08K1x",
        "outputId": "c3809044-b9e1-4a8f-c7b9-d19666e06fed"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation2 = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory2,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "5ESlXuAY8pSn"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation2.predict(input=\"What would be the best demo I can give in my free time\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "rE4HYG07ATks",
        "outputId": "cb17613d-9949-4444-ff80-c8c1ef5e8a9e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello\n",
            "AI: What's up\n",
            "Human: Not much, just hanging\n",
            "AI: Cool\n",
            "Human: What is on the schedule today?\n",
            "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
            "Human: What would be the best demo I can give in my free time\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A great demo to showcase in your free time could be a conversational AI application using LangChain. You could create a chatbot that interacts with users in a natural way, answering questions or providing recommendations based on user input. \\n\\nTo make it more engaging, you could integrate it with a real-world API, like a weather service or a movie database, allowing the chatbot to provide dynamic responses. For example, you could demonstrate how the chatbot can help users find movies based on their preferences or give them the current weather in their location.\\n\\nAdditionally, you could highlight the ease of building such applications with LangChain by showing how quickly you can set up the conversation flow and integrate various components. This would not only impress your customer but also illustrate the power and versatility of the tool in real-world applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory2.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1hOeLyzAXq2",
        "outputId": "2b6a32a8-80cb-473a-e716-42725fcdd771"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"System: The human greets the AI, and they exchange casual pleasantries. The human inquires about the day's schedule, and the AI outlines a series of events: a meeting at 8am with the product team requiring a PowerPoint presentation, time from 9am to 12pm to work on the LangChain project, a lunch at an Italian restaurant with a customer at noon to discuss the latest in AI, and a reminder to bring a laptop for a demo of the latest LLM.\\nHuman: What would be the best demo I can give in my free time\\nAI: A great demo to showcase in your free time could be a conversational AI application using LangChain. You could create a chatbot that interacts with users in a natural way, answering questions or providing recommendations based on user input. \\n\\nTo make it more engaging, you could integrate it with a real-world API, like a weather service or a movie database, allowing the chatbot to provide dynamic responses. For example, you could demonstrate how the chatbot can help users find movies based on their preferences or give them the current weather in their location.\\n\\nAdditionally, you could highlight the ease of building such applications with LangChain by showing how quickly you can set up the conversation flow and integrate various components. This would not only impress your customer but also illustrate the power and versatility of the tool in real-world applications.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GKfs9lIzAeLa"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains"
      ],
      "metadata": {
        "id": "KGIYEXY3Y6Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "fw4W0n8BY7s9"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model='gpt-4o-mini',temperature=0.9,api_key=os.getenv(\"OPEN_API_KEY\"))"
      ],
      "metadata": {
        "id": "l4NFjWoYZHAV"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"What is the best name to describe a company that makes {product}\")"
      ],
      "metadata": {
        "id": "5F0LEhJ0ZXid"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm,prompt=prompt_template)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e8USDpxZxwp",
        "outputId": "3e1953dd-e4c4-490c-c67a-954372521324"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-63-2db03ddad2cf>:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm,prompt=prompt_template)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Queen Size Sheet Set\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "b9D8dwhlZ_qv",
        "outputId": "ebd57d21-9cf2-4897-d53a-41ecc7ce9a5f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-97a624c538eb>:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  chain.run(\"Queen Size Sheet Set\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Choosing a name for a company that specializes in queen-size sheet sets can be both fun and strategic. Here are some suggestions that convey comfort, quality, and relevance to your product:\\n\\n1. **Queen's Comfort**\\n2. **Sheet Serenity**\\n3. **Royal Rest**\\n4. **Queenly Sleep**\\n5. **DreamySheets**\\n6. **Majestic Linens**\\n7. **Sovereign Sheets**\\n8. **Sleep Royalty**\\n9. **Perfectly Queen**\\n10. **LuxeSlumber**\\n\\nWhen selecting a name, consider factors like memorability, relevance, and availability of the domain name, especially if you plan to have an online presence.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain"
      ],
      "metadata": {
        "id": "bNDdAteLaDl2"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_1 = ChatPromptTemplate.from_template(\"What is the best name describe the company that makes {product} choose the best one\")\n",
        "chain_1 = LLMChain(llm=llm,prompt=prompt_template_1)\n",
        "\n",
        "prompt_template_2 = ChatPromptTemplate.from_template(\"Write 20 words description for the company: {company_name}\")\n",
        "chain_2 = LLMChain(llm=llm,prompt=prompt_template_2)"
      ],
      "metadata": {
        "id": "e3IyvJaDahvX"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_1,chain_2],verbose=True)"
      ],
      "metadata": {
        "id": "zYtGqi0JbJod"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain.run(\"Queen Size Sheet Set\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "GfP6nq4JbcwG",
        "outputId": "4076169c-1103-496a-c38c-56ea7a784f5c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mChoosing the best name for a company that specializes in Queen Size Sheet Sets involves considering factors like clarity, appeal, and branding potential. Here are a few options that could work well:\n",
            "\n",
            "1. **Queen Comfort Co.**\n",
            "2. **Royal Rest Textiles**\n",
            "3. **SereneSheets**\n",
            "4. **Dreamy Duvets**\n",
            "5. **Regal Slumber**\n",
            "\n",
            "Among these, **Queen Comfort Co.** effectively conveys the focus on queen-sized products and comfort, making it a strong choice.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mQueen Comfort Co. specializes in luxurious queen-sized sheet sets, offering ultimate comfort and style for a restful night's sleep.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Queen Comfort Co. specializes in luxurious queen-sized sheet sets, offering ultimate comfort and style for a restful night's sleep.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain"
      ],
      "metadata": {
        "id": "xKTRED77biEX"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review in English \\\n",
        "    Review: {Review}\"\n",
        ")\n",
        "chain_1 =LLMChain(llm=llm,prompt=prompt_1,output_key=\"English_Review\")\n",
        "\n",
        "prompt_2 = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence \\\n",
        "    English Review: {English_Review}\"\n",
        ")\n",
        "chain_2 = LLMChain(llm=llm,prompt=prompt_2,output_key=\"summary\")\n",
        "\n",
        "prompt_3 = ChatPromptTemplate.from_template(\n",
        "    \"What Language is the following review \\\n",
        "    Review: {Review}\"\n",
        ")\n",
        "chain_3 = LLMChain(llm=llm,prompt=prompt_3,output_key=\"language\")\n",
        "\n",
        "prompt_4 = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response of the Summary in the specified Language \\\n",
        "    Summary: {summary} Language: {language}\"\n",
        ")\n",
        "chain_4 = LLMChain(llm=llm,prompt=prompt_4,output_key=\"followup_message\")"
      ],
      "metadata": {
        "id": "ycgLQZ-kcUtc"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_1, chain_2, chain_3, chain_4],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\",\"summary\",\"followup_message\",\"language\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "UoHKYO7vcXtI"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review=\"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
        "overall_chain(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwi0IuSFzQgd",
        "outputId": "933d6647-a35d-46e4-c513-cda672af664f"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-80-39cc038682f7>:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  overall_chain(review)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
              " 'English_Review': \"Review: I find the taste mediocre. The mousse doesn't hold, it's strange. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\",\n",
              " 'summary': 'The reviewer describes the mousse as having a mediocre taste and a strange texture, suggesting it may be an old batch or counterfeit compared to the better-quality ones they purchase in stores.',\n",
              " 'followup_message': \"Réponse : Merci pour votre retour sur la mousse. Nous sommes désolés d'apprendre que votre expérience n’a pas été à la hauteur de vos attentes. Nous nous efforçons de garantir la meilleure qualité de nos produits, et votre commentaire nous aidera à améliorer nos standards. Pourriez-vous nous fournir plus de détails sur le produit que vous avez reçu afin que nous puissions enquêter davantage ? Nous apprécions votre avis et espérons pouvoir vous offrir une meilleure expérience à l'avenir.\",\n",
              " 'language': 'The review is in French.'}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "eo2yTgD10Yv0"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "NlY1-WWu3TP1"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n"
      ],
      "metadata": {
        "id": "JKPYMyZ83T6W"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "destination_chains = {}\n",
        "\n",
        "for p_info in prompt_infos:\n",
        "  name = p_info[\"name\"]\n",
        "  prompt = ChatPromptTemplate.from_template(p_info[\"prompt_template\"])\n",
        "  chain = LLMChain(llm=llm,prompt=prompt)\n",
        "  destination_chains[name] = chain\n",
        "\n",
        "destinations = [f'{p[\"name\"]} : {p[\"description\"]}' for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "print(destinations_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hy-_NKS4xVQ",
        "outputId": "d7bfe3b6-eb2e-4ca3-f7db-c50334293ff6"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "physics : Good for answering questions about physics\n",
            "math : Good for answering math questions\n",
            "History : Good for answering history questions\n",
            "computer science : Good for answering computer science questions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(prompt=default_prompt,llm=llm)"
      ],
      "metadata": {
        "id": "InNyLBen53aw"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text to the language model \\\n",
        "select the model prompt best suited for the input. You will be given names of \\\n",
        "the available prompts and the description of what prompt is best suited for. \\\n",
        "You may also revise the original input if you think revising ultimately leads \\\n",
        "to better response form the  language model\n",
        "<<FORMATTING>>\n",
        "Return the markdown code snippet with a JSON object formatted to look like\n",
        "```json\n",
        "{{{{\n",
        "  \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "  \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" must be one of the candidate prompt \\\n",
        "names specified below or it can be \"DEFAULT\" if input is not well suited \\\n",
        "with candidate prompts\n",
        "\n",
        "REMEMBER: \"next_inputs\" can be original input if you think no modifications \\\n",
        "are needed.\n",
        "\n",
        "<<CANDIDATE PROMPTS>>\n",
        "{destinations}\n",
        "\n",
        "<<INPUT>>\n",
        "{{input}}\n",
        "<<OUTPUT (remember to include ```json)>>>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4gQlq1iz6hyR"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser()\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm,router_prompt)"
      ],
      "metadata": {
        "id": "sN2Q2mxIFMpg"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = MultiPromptChain(\n",
        "    router_chain=router_chain,\n",
        "    destination_chains=destination_chains,\n",
        "    default_chain=default_chain,verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvqfufTUFtSp",
        "outputId": "83311896-87ea-4333-cedb-a76b8e214efb"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-95-f662c0d0c451>:1: LangChainDeprecationWarning: Use RunnableLambda to select from multiple prompt templates. See example in API reference: https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html\n",
            "  chain = MultiPromptChain(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"What is black body radiation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "zGixzjhtHhJO",
        "outputId": "7c1f63b8-c8d6-42ec-f050-8f574c89ca5a"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Black body radiation refers to the electromagnetic radiation emitted by an idealized object known as a \"black body.\" This theoretical object absorbs all incident radiation, regardless of frequency or angle, and reflects none. \\n\\nA black body in thermal equilibrium at a certain temperature emits radiation characterized by a specific spectrum, which depends solely on its temperature. This spectrum is described by Planck\\'s law, which shows that as the temperature increases, the intensity of radiation emitted at shorter wavelengths increases. \\n\\nKey features of black body radiation include:\\n\\n1. **Spectrum**: The radiation emitted spans a range of wavelengths, with a peak that shifts towards shorter wavelengths as the temperature rises (Wien\\'s displacement law).\\n  \\n2. **Temperature Dependence**: The total energy radiated increases with the fourth power of the temperature (Stefan-Boltzmann law), meaning hotter objects emit much more energy than cooler ones.\\n\\n3. **Quantum Mechanics**: The study of black body radiation was crucial in the development of quantum mechanics, particularly through Max Planck’s explanation of the observed spectrum, which introduced the concept of quantized energy levels.\\n\\nUnderstanding black body radiation helps explain many phenomena in thermodynamics, astrophysics, and quantum physics.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"what is 2 + 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "rUeL4N_THi2p",
        "outputId": "44b723af-6106-43f8-9c71-b44de442666c"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "math: {'input': 'what is 2 + 2'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Thank you for the compliment! Let's break down the question \\\\(2 + 2\\\\).\\n\\n1. Identify the numbers involved: We have the number 2 and another number 2.\\n2. Understand the operation: We are performing addition, which means we are combining these two numbers.\\n3. Combine the numbers: \\n\\n   \\\\[\\n   2 + 2 = 4\\n   \\\\]\\n\\nSo, the final answer is \\\\(4\\\\).\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"Why does every cell in our body contain DNA?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "1MX3YCLLHmmv",
        "outputId": "d4381f64-b7a8-4731-a46a-f25059230c99"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "None: {'input': 'Why does every cell in our body contain DNA?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Every cell in our body contains DNA because DNA serves as the fundamental blueprint for the development, functioning, growth, and reproduction of living organisms. Here are the primary reasons why DNA is present in nearly every cell:\\n\\n1. **Genetic Information**: DNA carries the genetic information necessary for the synthesis of proteins, which are essential for the structure, function, and regulation of the body's tissues and organs. Each gene within the DNA encodes instructions for making specific proteins.\\n\\n2. **Cell Function and Identity**: Each cell type in the body (e.g., muscle cells, nerve cells, skin cells) has the same DNA but may express different genes to perform its specific function. This differential gene expression is essential for the specialization of cells.\\n\\n3. **Development and Growth**: During the development of an organism from a single fertilized egg (zygote), DNA replication ensures that all daughter cells receive the same genetic material. This consistency is crucial for proper growth and development.\\n\\n4. **Repair and Maintenance**: DNA allows cells to repair themselves and regenerate. If damage occurs, the cell can use its DNA to guide the repair process or to produce new cells through processes like mitosis and meiosis.\\n\\n5. **Inheritance**: DNA is passed down from one generation to the next, ensuring that offspring inherit traits from their parents. This genetic continuity is vital for evolution and the survival of species.\\n\\n6. **Cellular Communication**: DNA influences how cells communicate and interact with each other, affecting processes like immune responses, hormonal signaling, and tissue repair.\\n\\nWhile most cells in the body contain DNA, there are some exceptions, such as mature red blood cells in humans, which lose their nuclei (and thus their DNA) as they mature. Despite these exceptions, the vast majority of cells rely on DNA to perform their essential functions.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gj766bp6Hp7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}